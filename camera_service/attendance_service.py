import cv2
from deepface import DeepFace
import json
import os
import numpy as np
from datetime import datetime, time
import time as time_module
import requests
import threading
from apscheduler.schedulers.background import BackgroundScheduler
import logging
import smtplib
from email.message import EmailMessage
from collections import deque
from typing import Optional, Dict
from dotenv import load_dotenv

# Load .env file from camera_service directory (same directory as this script)
env_path = os.path.join(os.path.dirname(__file__), '.env')
load_dotenv(dotenv_path=env_path)

try:
    from pinecone import Pinecone
except Exception as e:
    Pinecone = None

try:
    from deep_sort_realtime.deepsort_tracker import DeepSort
except Exception:
    DeepSort = None

# ============================================================================
# LOGGING SETUP - Only show important logs (WARNING level)
# ============================================================================

logging.basicConfig(
    level=logging.WARNING,
    format='%(levelname)s:%(name)s: %(message)s'
)
logger = logging.getLogger(__name__)

# ============================================================================
# CONFIGURATION
# ============================================================================

DATA_DIR = "../data"
BACKEND_API = "http://localhost:8000/api"

SIMILARITY_THRESHOLD = float(os.getenv("SIMILARITY_THRESHOLD", "0.45"))
MODEL = "ArcFace"
DETECTION_INTERVAL = 2.0
ATTENDANCE_COOLDOWN = 30  # Seconds cooldown between camera detections (database check handles duplicates)
TEST_MODE_ALWAYS_ACTIVE = False  # False = only mark during scheduled time, True = always mark
PROCESS_EVERY_N_FRAMES = 30  # Process every 30 frames (~1 time/sec) - attendance needs persistence, not frequency
FACE_EXTRACTION_INTERVAL = 10.0  # Cache face extraction for 10 seconds, reuse between frames (aggressive caching)
MODE_CHECK_INTERVAL = 0.5  # seconds (increased frequency for instant mode detection)
EXAM_DETECT_INTERVAL = 1  # seconds
PHONE_CONSEC_FRAMES = 1  # Instant detection - alert on first frame phone detected
EXAM_ALERT_COOLDOWN = 30  # seconds (reduced from 60 to allow more frequent alerts)
PHONE_CONFIDENCE_THRESHOLD = 0.15  # VERY LOW threshold - detect even partial phone visibility (was 0.5)

# ============================================================================
# EXAM-GRADE PHONE DETECTION - BALANCED VALIDATION (PRACTICAL)
# ============================================================================
PHONE_MAX_AREA_RATIO = 0.25  # Max 25% of frame - rejects laptop/paper (relaxed for partial phone)
PHONE_MIN_AREA_RATIO = 0.002  # Min 0.2% of frame - allow very small partial phone
PHONE_ASPECT_RATIO_MIN = 0.3  # Min aspect ratio (relaxed for partial view)
PHONE_ASPECT_RATIO_MAX = 3.5  # Max aspect ratio (relaxed)
PHONE_RECTANGULARITY_MIN = 0.60  # Relaxed - phone partial view may not be perfect rectangle
PHONE_EMISSIVE_RATIO = 1.08  # Center brightness 8% > border (relaxed, screen may be dark)
PHONE_MOTION_THRESHOLD = 3.0  # Relaxed motion threshold
PHONE_STATIC_REJECT_SECONDS = 5.0  # Allow 5 seconds static (user may hold steady)
PHONE_MIN_VALIDATION_SCORE = 10  # Minimum score to confirm phone (scoring system)

FACE_SURE_THRESHOLD = 0.8
FACE_MAYBE_THRESHOLD = 0.5
FRAME_WIDTH = int(os.getenv("FRAME_WIDTH", "1280"))
FRAME_HEIGHT = int(os.getenv("FRAME_HEIGHT", "720"))
DISPLAY_WIDTH = 960  # ‚úÖ FIX 4: Reduce display resolution (separate from processing)
DISPLAY_HEIGHT = 540
FACE_DETECTOR_BACKEND = os.getenv("FACE_DETECTOR_BACKEND", "retinaface")
FACE_DETECTOR_FALLBACK = None  # ‚úÖ FIX 5: NO fallback to MTCNN (prevents double detection)
FACE_DET_CONFIDENCE = float(os.getenv("FACE_DET_CONFIDENCE", "0.5"))
FACE_DET_UPSCALE = float(os.getenv("FACE_DET_UPSCALE", "1.5"))
MIN_FACE_SIZE = int(os.getenv("MIN_FACE_SIZE", "20"))
PINECONE_ENABLED = os.getenv("PINECONE_ENABLED", "1") == "1"
PINECONE_API_KEY = os.getenv("PINECONE_API_KEY", "")
PINECONE_INDEX_NAME = os.getenv("PINECONE_INDEX_NAME", "face-recognition")
PINECONE_ENVIRONMENT = os.getenv("PINECONE_ENVIRONMENT", "us-east-1-aws")

TRACKING_ENABLED = os.getenv("TRACKING_ENABLED", "1") == "1"
TRACK_MIN_SECONDS = float(os.getenv("TRACK_MIN_SECONDS", "3.0"))
TRACK_MIN_HITS = int(os.getenv("TRACK_MIN_HITS", "3"))
TRACK_IOU_MATCH = float(os.getenv("TRACK_IOU_MATCH", "0.3"))
TRACK_STALE_SECONDS = float(os.getenv("TRACK_STALE_SECONDS", "2.0"))
LIVENESS_ENABLED = os.getenv("LIVENESS_ENABLED", "1") == "1"
LIVENESS_WINDOW_SECONDS = float(os.getenv("LIVENESS_WINDOW_SECONDS", "3.0"))
LIVENESS_MIN_MOVEMENT_PX = float(os.getenv("LIVENESS_MIN_MOVEMENT_PX", "8.0"))

# ============================================================================
# UTILITIES
# ============================================================================

def load_json_file(filepath):
    """Load JSON file"""
    if not os.path.exists(filepath):
        return {}
    try:
        with open(filepath, 'r') as f:
            return json.load(f)
    except Exception as e:
        logger.error(f"Error loading {filepath}: {e}")
        return {}

def load_from_data_dir(filename):
    """Load from data directory"""
    return load_json_file(os.path.join(DATA_DIR, filename))

# ============================================================================
# STUDENT FACE DATABASE
# ============================================================================

class FaceDatabase:
    def __init__(self):
        self.students = {}
        self.embeddings = {}
        self.pinecone_index = None
        self.pinecone_client = None
        self._init_pinecone()
        self.load_students()

    def _init_pinecone(self):
        """Initialize Pinecone client and connect to index (Pinecone 3.x API)"""
        if not PINECONE_ENABLED or Pinecone is None or not PINECONE_API_KEY:
            logger.info("‚ö†Ô∏è Pinecone not enabled or API key missing")
            self.pinecone_client = None
            self.pinecone_index = None
            return

        try:
            # Initialize Pinecone with new 3.x API
            self.pinecone_client = Pinecone(api_key=PINECONE_API_KEY)
            logger.info("‚úÖ Pinecone client created")

            # Check if index exists
            indexes = self.pinecone_client.list_indexes()
            index_names = [idx.name for idx in indexes.indexes] if hasattr(indexes, 'indexes') else []
            
            if PINECONE_INDEX_NAME not in index_names:
                logger.warning(f"‚ö†Ô∏è Pinecone index '{PINECONE_INDEX_NAME}' not found. Please create it manually in Pinecone console:")
                logger.warning(f"   - Index name: {PINECONE_INDEX_NAME}")
                logger.warning(f"   - Dimensions: 512")
                logger.warning(f"   - Metric: cosine")
                logger.warning(f"   - Region: us-east-1 (AWS)")
                self.pinecone_client = None
                self.pinecone_index = None
                return
            else:
                logger.info(f"‚úÖ Found existing Pinecone index: {PINECONE_INDEX_NAME}")

            # Connect to existing index
            self.pinecone_index = self.pinecone_client.Index(PINECONE_INDEX_NAME)
            logger.info(f"‚úÖ Pinecone initialized successfully")
        except Exception as e:
            logger.error(f"‚ùå Failed to initialize Pinecone: {e}")
            self.pinecone_client = None
            self.pinecone_index = None

    def load_students(self):
        """Load student embeddings from MongoDB via backend API (PRIMARY SOURCE)"""
        try:
            response = requests.get(f"{BACKEND_API}/students", timeout=5)
            if response.status_code == 200:
                students_list = response.json()
                
                # Convert list format to roll_number keyed format
                for student in students_list:
                    roll = student.get("roll_number")
                    if roll:
                        self.students[roll] = student
                        
                        # Extract embedding if present
                        if "embedding" in student and student["embedding"]:
                            try:
                                self.embeddings[roll] = np.array(student["embedding"])
                            except Exception as e:
                                logger.warning(f"Could not process embedding for {roll}: {e}")
                
                logger.info(f"‚úÖ Loaded {len(self.students)} students from MongoDB")
                return
        except Exception as e:
            logger.error(f"‚ùå Failed to fetch from MongoDB API: {e}")
            raise


    
    def get_student_by_roll(self, roll_number):
        """Get student info by roll number"""
        return self.students.get(roll_number)
    
    def get_all_embeddings(self):
        """Get all student embeddings"""
        return self.embeddings

    def push_embedding_to_pinecone(self, roll_number: str, embedding: np.ndarray) -> bool:
        """Push single embedding to Pinecone"""
        if self.pinecone_index is None:
            return False

        try:
            embedding_list = embedding.astype("float32").tolist()
            self.pinecone_index.upsert(
                vectors=[(roll_number, embedding_list)],
                namespace="face-recognition"
            )
            return True
        except Exception as e:
            logger.error(f"‚ùå Failed to push embedding to Pinecone for {roll_number}: {e}")
            return False

    def delete_embedding_from_pinecone(self, roll_number: str) -> bool:
        """Delete embedding from Pinecone"""
        if self.pinecone_index is None:
            return False

        try:
            self.pinecone_index.delete(ids=[roll_number], namespace="face-recognition")
            return True
        except Exception as e:
            logger.error(f"‚ùå Failed to delete embedding from Pinecone for {roll_number}: {e}")
            return False

    def search_best(self, embedding: np.ndarray) -> Optional[Dict]:
        """Search best matching student using Pinecone (fallback to local embeddings)"""
        if self.pinecone_index is not None:
            try:
                import time
                query_embedding = embedding.astype("float32").tolist()
                logger.info(f"üîç Searching Pinecone with embedding vector (dim={len(query_embedding)})")
                
                start_time = time.time()
                logger.info(f"‚è≥ About to call pinecone_index.query()...")
                
                # Pinecone 3.x API query syntax
                results = self.pinecone_index.query(
                    vector=query_embedding,
                    top_k=1,
                    include_metadata=True
                )
                
                logger.info(f"‚úÖ pinecone_index.query() returned!")
                elapsed = time.time() - start_time
                
                logger.info(f"üìä Pinecone query took {elapsed:.2f}s")
                logger.info(f"üìä Pinecone response type: {type(results)}")
                logger.info(f"üìä Has matches attr: {hasattr(results, 'matches')}")

                # Parse Pinecone 3.x response format
                if hasattr(results, 'matches') and results.matches and len(results.matches) > 0:
                    match = results.matches[0]
                    roll_number = match.id
                    similarity = float(match.score)
                    student = self.get_student_by_roll(roll_number)

                    if student:
                        logger.info(f"‚úÖ Pinecone match: {student.get('name')} (similarity: {similarity:.3f})")
                        return {
                            "roll_number": roll_number,
                            "name": student.get("name"),
                            "similarity": similarity
                        }
                    else:
                        logger.warning(f"‚ö†Ô∏è Pinecone returned ID {roll_number} but student not found in memory")
                        return None
                        
                logger.warning(f"‚ö†Ô∏è Pinecone returned no matches")
                return None
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Pinecone search failed: {type(e).__name__}: {e}, falling back to local search")

        if not self.embeddings:
            return None

        best_roll = None
        best_similarity = -1
        query_norm = np.linalg.norm(embedding)

        if query_norm == 0:
            return None

        for roll_number, stored_embedding in self.embeddings.items():
            stored_norm = np.linalg.norm(stored_embedding)
            if stored_norm == 0:
                continue

            similarity = np.dot(embedding, stored_embedding) / (query_norm * stored_norm)

            if similarity > best_similarity:
                best_similarity = similarity
                best_roll = roll_number

        if best_roll is None:
            return None

        student = self.get_student_by_roll(best_roll)
        if not student:
            return None

        return {
            "roll_number": best_roll,
            "name": student.get("name"),
            "similarity": float(best_similarity)
        }

# ============================================================================
# CAMERA ATTENDANCE
# ============================================================================

class CameraAttendance:
    def __init__(self, camera_id, camera_name, batch_id):
        self.camera_id = camera_id
        self.camera_name = camera_name
        self.batch_id = batch_id
        self.face_db = FaceDatabase()
        self.last_marked = {}  # {"roll_number": timestamp}
        self.is_recording = False
        self.last_schedule_log = None
        self.cached_mode = "NORMAL"
        self.last_mode_check = None
        self.phone_detect_count = 0
        self.last_alert_time = None
        self.last_exam_check = None
        self.last_detected_faces = []  # Cache for detected faces
        self.face_cache_time = None  # Timestamp of last face detection
        
        # Phone tracking state for temporal validation
        self.phone_bbox_history = deque(maxlen=10)  # Last 10 phone detections
        self.phone_first_seen = None  # When phone was first detected
        self.phone_last_motion = None  # Last time phone moved
        self.FACE_CACHE_DURATION = 3.0  # Keep displaying face for 3 seconds (for smooth display)
        self.last_face_extraction_time = None  # Track when we last extracted faces
        self.cached_face_results = []  # Cache extracted face results
        self.tracker = self._init_tracker()
        self.track_state = {}  # {track_id: {"embedding": ..., "marked": True, "student_roll": ...}}
        
        # ‚úÖ FIX: Warm up DeepFace model cache to avoid 3-10 sec delay on first use
        logger.info("üöÄ Warming up DeepFace ArcFace model cache...")
        self.arcface_model = None
        try:
            import numpy as np
            # Create a dummy image to force DeepFace to load and cache the model
            dummy_img = np.random.randint(0, 255, (112, 112, 3), dtype=np.uint8)
            _ = DeepFace.represent(dummy_img, model_name=MODEL, enforce_detection=False)
            logger.warning("‚úÖ ArcFace Model: LOADED & CACHED (face recognition ready)")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Could not warm up ArcFace cache: {e}, first embedding may be slow")
        
        # ‚úÖ FIX 5: Preload YOLO model on init (NOT at runtime) to avoid freeze
        logger.warning("üîÑ Initializing Phone Detection (YOLO)...")
        try:
            from ultralytics import YOLO
            self.yolo_model = YOLO("yolov8n.pt")
            logger.warning("‚úÖ YOLO Model: LOADED & READY (phone detection active)")
        except Exception as e:
            logger.warning(f"Failed to preload YOLO model: {e}")
            self.yolo_model = None
        
        # ‚úÖ FIX 1: Background thread for AI processing
        self.latest_result = None  # Latest AI result (used by display thread)
        self.ai_frame_buffer = None  # Buffer for frame to process
        self.ai_lock = threading.Lock()  # Thread-safe access to latest_result

    def _ai_worker_thread(self, frame, frame_count):
        """üî• FIX 1: Background thread for AI processing
        
        Camera thread never waits for AI.
        AI runs async and updates self.latest_result.
        """
        try:
            logger.info(f"üß† AI worker thread started for frame {frame_count}")
            result = self.process_frame(frame)
            with self.ai_lock:
                self.latest_result = result
            logger.info(f"‚úÖ AI result updated at frame {frame_count}: status={result.get('status')}")
        except Exception as e:
            logger.error(f"‚ùå Error in AI worker thread: {e}")
            import traceback
            logger.error(traceback.format_exc())

    def get_camera_mode(self):
        """Fetch camera mode from backend with caching"""
        now = datetime.now()
        if self.last_mode_check and (now - self.last_mode_check).total_seconds() < MODE_CHECK_INTERVAL:
            return self.cached_mode

        try:
            response = requests.get(f"{BACKEND_API}/camera-mode/{self.camera_id}", timeout=3)
            if response.status_code == 200:
                data = response.json()
                mode = data.get("mode", "NORMAL")
                self.cached_mode = mode
        except Exception as e:
            logger.warning(f"Could not fetch camera mode: {e}")

        self.last_mode_check = now
        return self.cached_mode

    def send_exam_alert(self, subject_id, time_slot):
        """Send exam alert email if SMTP is configured"""
        smtp_host = os.getenv("SMTP_HOST")
        smtp_port = int(os.getenv("SMTP_PORT", "587"))
        smtp_user = os.getenv("SMTP_USER")
        smtp_pass = os.getenv("SMTP_PASS")
        email_to = os.getenv("EXAM_ALERT_EMAIL_TO")
        email_from = os.getenv("EXAM_ALERT_EMAIL_FROM", smtp_user)

        if not smtp_host or not smtp_user or not smtp_pass or not email_to:
            logger.warning("Exam alert email not configured. Set SMTP_HOST, SMTP_USER, SMTP_PASS, EXAM_ALERT_EMAIL_TO")
            return

        msg = EmailMessage()
        msg["Subject"] = "Exam Cheating Alert"
        msg["From"] = email_from
        msg["To"] = email_to
        msg.set_content(
            f"Mobile phone detected.\n"
            f"Room: {self.camera_name}\n"
            f"Camera ID: {self.camera_id}\n"
            f"Subject: {subject_id}\n"
            f"Time Slot: {time_slot}\n"
            f"Time: {datetime.now().strftime('%H:%M:%S')}\n\n"
            f"Please verify."
        )

        try:
            with smtplib.SMTP(smtp_host, smtp_port) as server:
                server.starttls()
                server.login(smtp_user, smtp_pass)
                server.send_message(msg)
            logger.info("üìß Exam alert email sent")
        except Exception as e:
            logger.error(f"Failed to send exam alert email: {e}")

    def _validate_phone_size(self, bbox, frame_shape):
        """RULE 1: Size constraint - phone is small, paper/laptop is large"""
        x1, y1, x2, y2 = bbox
        width = x2 - x1
        height = y2 - y1
        area = width * height
        
        frame_height, frame_width = frame_shape[:2]
        frame_area = frame_width * frame_height
        area_ratio = area / frame_area
        
        # Reject if too large (laptop/paper) or too small (artifact)
        if area_ratio > PHONE_MAX_AREA_RATIO:
            logger.debug(f"‚ùå Size filter: Too large ({area_ratio:.1%} > {PHONE_MAX_AREA_RATIO:.1%}) - likely laptop/paper")
            return False, "too_large"
        if area_ratio < PHONE_MIN_AREA_RATIO:
            logger.debug(f"‚ùå Size filter: Too small ({area_ratio:.1%} < {PHONE_MIN_AREA_RATIO:.1%}) - artifact")
            return False, "too_small"
        
        # Check aspect ratio (phone is ~9:16 or 16:9, paper is irregular)
        aspect = width / height if height > 0 else 0
        aspect_inv = height / width if width > 0 else 0
        aspect_check = max(aspect, aspect_inv)
        
        if aspect_check < PHONE_ASPECT_RATIO_MIN or aspect_check > PHONE_ASPECT_RATIO_MAX:
            logger.debug(f"‚ùå Aspect ratio filter: {aspect_check:.2f} not phone-like")
            return False, "bad_aspect"
        
        logger.debug(f"‚úÖ Size OK: {area_ratio:.1%} of frame, aspect={aspect_check:.2f}")
        return True, area_ratio
    
    def _validate_phone_edges(self, bbox, frame):
        """RULE 2: Edge sharpness - phone has sharp rectangular edges, paper is soft/irregular"""
        try:
            x1, y1, x2, y2 = [int(v) for v in bbox]
            
            # Extract region with margin
            margin = 20
            x1_m = max(0, x1 - margin)
            y1_m = max(0, y1 - margin)
            x2_m = min(frame.shape[1], x2 + margin)
            y2_m = min(frame.shape[0], y2 + margin)
            
            roi = frame[y1_m:y2_m, x1_m:x2_m]
            if roi.size == 0:
                return False, "empty_roi"
            
            # Convert to grayscale and find edges
            gray = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)
            edges = cv2.Canny(gray, 50, 150)
            
            # Find contours
            contours, _ = cv2.findContours(edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
            if not contours:
                return False, "no_contours"
            
            # Get largest contour
            largest_contour = max(contours, key=cv2.contourArea)
            area = cv2.contourArea(largest_contour)
            if area < 100:
                return False, "contour_too_small"
            
            # Check rectangularity
            perimeter = cv2.arcLength(largest_contour, True)
            if perimeter == 0:
                return False, "zero_perimeter"
            
            approx = cv2.approxPolyDP(largest_contour, 0.02 * perimeter, True)
            rect_area = cv2.minAreaRect(largest_contour)
            rect_width, rect_height = rect_area[1]
            rect_box_area = rect_width * rect_height
            
            rectangularity = area / rect_box_area if rect_box_area > 0 else 0
            
            if rectangularity < PHONE_RECTANGULARITY_MIN:
                logger.debug(f"‚ùå Edge filter: Not rectangular enough ({rectangularity:.2f} < {PHONE_RECTANGULARITY_MIN})")
                return False, "not_rectangular"
            
            logger.debug(f"‚úÖ Edges OK: rectangularity={rectangularity:.2f}")
            return True, rectangularity
        except Exception as e:
            logger.debug(f"Edge validation error: {e}")
            return False, "error"
    
    def _validate_phone_emissive(self, bbox, frame):
        """RULE 3: Emissive vs reflective - phone screen glows from center, paper reflects uniformly"""
        try:
            x1, y1, x2, y2 = [int(v) for v in bbox]
            roi = frame[y1:y2, x1:x2]
            
            if roi.size == 0:
                return False, "empty_roi"
            
            # Convert to grayscale
            gray = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)
            h, w = gray.shape
            
            # Define center and border regions
            center_x1, center_y1 = w // 4, h // 4
            center_x2, center_y2 = 3 * w // 4, 3 * h // 4
            
            center_region = gray[center_y1:center_y2, center_x1:center_x2]
            
            # Border is outer 20% of each edge
            border_thickness = max(2, min(h, w) // 10)
            top_border = gray[:border_thickness, :]
            bottom_border = gray[-border_thickness:, :]
            left_border = gray[:, :border_thickness]
            right_border = gray[:, -border_thickness:]
            
            if center_region.size == 0:
                return False, "empty_center"
            
            # Calculate average brightness
            center_brightness = np.mean(center_region)
            border_brightness = np.mean([
                np.mean(top_border) if top_border.size > 0 else 0,
                np.mean(bottom_border) if bottom_border.size > 0 else 0,
                np.mean(left_border) if left_border.size > 0 else 0,
                np.mean(right_border) if right_border.size > 0 else 0
            ])
            
            # Phone screen: center is brighter (emissive)
            # Paper: uniform brightness (reflective)
            emissive_ratio = center_brightness / border_brightness if border_brightness > 10 else 0
            
            if emissive_ratio < PHONE_EMISSIVE_RATIO:
                logger.debug(f"‚ùå Emissive filter: Not glowing from center ({emissive_ratio:.2f} < {PHONE_EMISSIVE_RATIO}) - likely paper")
                return False, "reflective"
            
            logger.debug(f"‚úÖ Emissive OK: center={center_brightness:.0f}, border={border_brightness:.0f}, ratio={emissive_ratio:.2f}")
            return True, emissive_ratio
        except Exception as e:
            logger.debug(f"Emissive validation error: {e}")
            return False, "error"
    
    def _validate_phone_motion(self, bbox):
        """RULE 4: Temporal behavior - phone moves (hand jitter), paper is static on desk"""
        try:
            now = datetime.now()
            x1, y1, x2, y2 = bbox
            center_x = (x1 + x2) / 2
            center_y = (y1 + y2) / 2
            
            # Add to history
            self.phone_bbox_history.append((now, (center_x, center_y)))
            
            if not self.phone_first_seen:
                self.phone_first_seen = now
            
            # Need at least 2 detections to check motion
            if len(self.phone_bbox_history) < 2:
                return True, "first_detection"  # Give benefit of doubt
            
            # Check if phone has moved in last few frames
            recent_positions = [pos for _, pos in list(self.phone_bbox_history)[-5:]]
            if len(recent_positions) < 2:
                return True, "insufficient_history"
            
            # Calculate max distance moved
            max_movement = 0
            for i in range(len(recent_positions) - 1):
                x1_pos, y1_pos = recent_positions[i]
                x2_pos, y2_pos = recent_positions[i + 1]
                dist = ((x2_pos - x1_pos) ** 2 + (y2_pos - y1_pos) ** 2) ** 0.5
                max_movement = max(max_movement, dist)
            
            # If phone has been static for too long, reject (paper on desk)
            if max_movement < PHONE_MOTION_THRESHOLD:
                time_static = (now - self.phone_first_seen).total_seconds()
                if time_static > PHONE_STATIC_REJECT_SECONDS:
                    logger.debug(f"‚ùå Motion filter: Static for {time_static:.1f}s (movement={max_movement:.1f}px) - likely paper on desk")
                    return False, "static_too_long"
            else:
                # Phone moved, update last motion time
                self.phone_last_motion = now
                self.phone_first_seen = now  # Reset static timer
            
            logger.debug(f"‚úÖ Motion OK: movement={max_movement:.1f}px")
            return True, max_movement
        except Exception as e:
            logger.debug(f"Motion validation error: {e}")
            return True, "error"  # Give benefit of doubt on error
    
    def detect_phone_in_frame(self, frame):
        """EXAM-GRADE phone detection with SCORING SYSTEM (balanced - detects phones, rejects paper/laptop)"""
        try:
            if self.yolo_model is None:
                logger.warning("YOLO model not available for phone detection")
                return False, 0.0

            best_confidence = 0.0
            best_score = 0
            frame_height, frame_width = frame.shape[:2]
            
            # Stage 1: YOLO detection (initial candidate)
            results = self.yolo_model(frame, verbose=False, conf=PHONE_CONFIDENCE_THRESHOLD, imgsz=640)
            
            for result in results:
                for box in result.boxes:
                    cls_id = int(box.cls[0])
                    cls_name = result.names.get(cls_id, "")
                    confidence = float(box.conf[0])
                    
                    # ONLY process "cell phone" class
                    if cls_name != "cell phone" or confidence < PHONE_CONFIDENCE_THRESHOLD:
                        continue
                    
                    # Get bounding box
                    x1, y1, x2, y2 = box.xyxy[0].tolist()
                    bbox = (x1, y1, x2, y2)
                    
                    logger.warning(f"üì± YOLO detected: confidence={confidence:.1%}, bbox={[int(v) for v in bbox]}")
                    
                    # ============ SCORING SYSTEM (NOT ALL-OR-NOTHING) ============
                    validation_score = 0
                    validation_details = []
                    
                    # Stage 2: Size constraint (MANDATORY - critical for laptop/paper rejection)
                    size_ok, size_info = self._validate_phone_size(bbox, frame.shape)
                    if not size_ok:
                        logger.warning(f"   ‚ùå Size check FAILED: {size_info} - HARD REJECT (likely laptop/paper)")
                        continue  # Hard reject if too large (laptop/paper)
                    
                    # Size passed - add base score
                    validation_score += 10
                    if isinstance(size_info, float):
                        validation_details.append(f"size={size_info:.1%}")
                    else:
                        validation_details.append(f"size=OK")
                    
                    # Stage 3: Edge sharpness (OPTIONAL - +3 points)
                    edges_ok, edge_info = self._validate_phone_edges(bbox, frame)
                    if edges_ok:
                        validation_score += 3
                        validation_details.append(f"edges={edge_info:.2f}")
                    else:
                        validation_details.append(f"edges=fail({edge_info})")
                    
                    # Stage 4: Emissive test (OPTIONAL - +4 points, strong signal)
                    emissive_ok, emissive_info = self._validate_phone_emissive(bbox, frame)
                    if emissive_ok:
                        validation_score += 4
                        validation_details.append(f"emissive={emissive_info:.2f}")
                    else:
                        validation_details.append(f"emissive=fail({emissive_info})")
                    
                    # Stage 5: Motion/liveness check (OPTIONAL - +2 points)
                    motion_ok, motion_info = self._validate_phone_motion(bbox)
                    if motion_ok:
                        validation_score += 2
                        if isinstance(motion_info, (int, float)):
                            validation_details.append(f"motion={motion_info:.1f}px")
                        else:
                            validation_details.append(f"motion={motion_info}")
                    else:
                        validation_details.append(f"motion=fail({motion_info})")
                    
                    # YOLO confidence bonus (high confidence = more likely real phone)
                    if confidence > 0.4:
                        validation_score += 3
                        validation_details.append("high_conf_bonus")
                    
                    # Decision: Score >= 10 = PHONE DETECTED
                    # Score 10-12 = Size passed (basic phone)
                    # Score 13-15 = Size + 1 feature (confident)
                    # Score 16-19 = Size + 2-3 features (very confident)
                    
                    details_str = " | ".join(validation_details)
                    
                    if validation_score >= PHONE_MIN_VALIDATION_SCORE:
                        best_confidence = max(best_confidence, confidence)
                        best_score = max(best_score, validation_score)
                        
                        if validation_score >= 16:
                            logger.warning(f"üö® PHONE CONFIRMED (VERY HIGH CONFIDENCE): score={validation_score}/19 | {details_str}")
                        elif validation_score >= 13:
                            logger.warning(f"üö® PHONE CONFIRMED (HIGH CONFIDENCE): score={validation_score}/19 | {details_str}")
                        else:
                            logger.warning(f"üö® PHONE DETECTED (BASIC): score={validation_score}/19 | {details_str}")
                        
                        return True, best_confidence
                    else:
                        logger.warning(f"   ‚ö†Ô∏è  Low score: {validation_score}/19 (need {PHONE_MIN_VALIDATION_SCORE}+) | {details_str}")
            
            # No phone passed validation
            logger.debug(f"üìµ No phone confirmed (all candidates scored too low)")
            
            # Reset tracking if no phone detected
            if len(self.phone_bbox_history) == 0 or \
               (len(self.phone_bbox_history) > 0 and 
                (datetime.now() - self.phone_bbox_history[-1][0]).total_seconds() > 2.0):
                self.phone_bbox_history.clear()
                self.phone_first_seen = None
                self.phone_last_motion = None
            
            return False, best_confidence
            
        except Exception as e:
            logger.error(f"‚ùå Phone detection error: {e}")
            import traceback
            logger.error(traceback.format_exc())
            return False, 0.0
    

    def handle_exam_frame(self, frame, schedule):
        """Handle exam mode logic"""
        now = datetime.now()
        if self.last_exam_check and (now - self.last_exam_check).total_seconds() < EXAM_DETECT_INTERVAL:
            return {"status": "exam_monitoring"}
        self.last_exam_check = now

        detected, phone_confidence = self.detect_phone_in_frame(frame)
        logger.debug(f"üîç Phone detection: detected={detected}, confidence={phone_confidence:.2%}, count={self.phone_detect_count}")
        
        if detected:
            self.phone_detect_count += 1
            logger.warning(f"üö® PHONE DETECTED IN EXAM: confidence={phone_confidence:.1%} - SENDING ALERT NOW!")
        else:
            self.phone_detect_count = 0

        if self.phone_detect_count >= PHONE_CONSEC_FRAMES:
            now = datetime.now()
            if not self.last_alert_time or (now - self.last_alert_time).total_seconds() > EXAM_ALERT_COOLDOWN:
                time_slot = f"{schedule.get('start_time').strftime('%H:%M')}-{schedule.get('end_time').strftime('%H:%M')}"
                logger.warning(f"üö® INSTANT ALERT: Mobile phone detected in {schedule.get('subject_id')} | Time: {time_slot} | Confidence: {phone_confidence:.1%}")
                self.send_exam_alert(schedule.get("subject_id"), time_slot)
                self.save_violation_to_backend(schedule, frame, phone_confidence)
                self.last_alert_time = now
            return {"status": "exam_alert"}

        if detected:
            return {"status": "phone_detected"}
        return {"status": "exam_monitoring"}
    
    def get_best_face_match(self, frame):
        """Get best face match from frame with similarity score"""
        try:
            embeddings_db = self.face_db.get_all_embeddings()
            if not embeddings_db:
                return None

            faces = self._extract_faces(frame)
            if not faces:
                return None

            best_match = None
            best_similarity = -1.0

            for face in faces:
                face_img = face.get("face")
                if face_img is None:
                    continue

                frame_embedding = self._compute_embedding(face_img)
                match = self._best_match_from_embedding(frame_embedding)
                if not match:
                    continue

                similarity = match.get("similarity", -1.0)
                if similarity > best_similarity:
                    best_similarity = similarity
                    best_match = match

            return best_match
        except Exception as e:
            logger.debug(f"Face match failed: {e}")
            return None

    def _extract_faces(self, frame):
        """Extract faces with optional upscaling for distant/partial faces"""
        scale = FACE_DET_UPSCALE if FACE_DET_UPSCALE > 1.0 else 1.0
        if scale > 1.0:
            frame_scaled = cv2.resize(
                frame,
                None,
                fx=scale,
                fy=scale,
                interpolation=cv2.INTER_LINEAR
            )
        else:
            frame_scaled = frame

        try:
            faces = DeepFace.extract_faces(
                img_path=frame_scaled,
                detector_backend=FACE_DETECTOR_BACKEND,
                enforce_detection=False,
                align=True
            )
        except Exception as e:
            logger.debug(f"Face extraction failed with {FACE_DETECTOR_BACKEND}: {e}")
            faces = []

        if not faces and FACE_DETECTOR_FALLBACK:
            try:
                faces = DeepFace.extract_faces(
                    img_path=frame_scaled,
                    detector_backend=FACE_DETECTOR_FALLBACK,
                    enforce_detection=False,
                    align=True
                )
            except Exception as e:
                logger.debug(f"Face extraction failed with {FACE_DETECTOR_FALLBACK}: {e}")
                return []

        results = []
        for face in faces or []:
            confidence = float(face.get("confidence", 0.0))
            facial_area = face.get("facial_area") or {}
            x = int(facial_area.get("x", 0))
            y = int(facial_area.get("y", 0))
            w = int(facial_area.get("w", 0))
            h = int(facial_area.get("h", 0))

            if scale > 1.0:
                x = int(x / scale)
                y = int(y / scale)
                w = int(w / scale)
                h = int(h / scale)

            if w < MIN_FACE_SIZE or h < MIN_FACE_SIZE:
                continue
            if confidence < FACE_DET_CONFIDENCE:
                continue

            results.append({
                "face": face.get("face"),
                "confidence": confidence,
                "x": x,
                "y": y,
                "w": w,
                "h": h
            })

        return results

    def _init_tracker(self):
        if not TRACKING_ENABLED or DeepSort is None:
            return None

        return DeepSort(
            max_age=30,
            n_init=2,
            max_iou_distance=0.7
        )

    def _compute_embedding(self, face_img):
        import time
        start_time = time.time()
        
        # DeepFace caches the model internally, so after warm-up this should be fast
        result = DeepFace.represent(face_img, model_name=MODEL, enforce_detection=False)
        
        elapsed = time.time() - start_time
        logger.info(f"‚ö° Embedding computed in {elapsed:.2f}s")
        
        if not result:
            return None
        return np.array(result[0]["embedding"]) 

    def _best_match_from_embedding(self, embedding):
        if embedding is None:
            return None

        # Try Pinecone first
        pinecone_match = self.face_db.search_best(embedding)
        if pinecone_match:
            similarity = pinecone_match.get("similarity", 0)
            logger.info(f"üìç Pinecone result: {pinecone_match.get('name')} (similarity={similarity:.3f}, threshold={SIMILARITY_THRESHOLD})")
            if similarity >= SIMILARITY_THRESHOLD:
                logger.info(f"‚úÖ Match passes threshold!")
                return pinecone_match
            else:
                logger.warning(f"‚ö†Ô∏è Match below threshold ({similarity:.3f} < {SIMILARITY_THRESHOLD})")
                return None

        # Fallback to local embeddings
        logger.info(f"üìÇ Pinecone empty, trying local embeddings...")
        best_similarity = -1.0
        best_roll = None
        for roll_number, student_embedding in self.face_db.get_all_embeddings().items():
            similarity = np.dot(embedding, student_embedding) / (
                np.linalg.norm(embedding) * np.linalg.norm(student_embedding)
            )
            if similarity > best_similarity:
                best_similarity = similarity
                best_roll = roll_number

        if best_roll is None:
            logger.warning(f"‚ùå No match found in local embeddings")
            return None

        student = self.face_db.get_student_by_roll(best_roll)
        logger.info(f"üìç Local match: {student.get('name')} (similarity={best_similarity:.3f}, threshold={SIMILARITY_THRESHOLD})")
        
        if best_similarity >= SIMILARITY_THRESHOLD:
            logger.info(f"‚úÖ Local match passes threshold!")
            return {
                "roll_number": best_roll,
                "name": student.get("name") if student else None,
                "similarity": float(best_similarity)
            }
        else:
            logger.warning(f"‚ö†Ô∏è Local match below threshold ({best_similarity:.3f} < {SIMILARITY_THRESHOLD})")
            return None

    def _iou(self, box_a, box_b):
        ax1, ay1, ax2, ay2 = box_a
        bx1, by1, bx2, by2 = box_b
        inter_x1 = max(ax1, bx1)
        inter_y1 = max(ay1, by1)
        inter_x2 = min(ax2, bx2)
        inter_y2 = min(ay2, by2)
        inter_w = max(0, inter_x2 - inter_x1)
        inter_h = max(0, inter_y2 - inter_y1)
        inter_area = inter_w * inter_h
        area_a = max(0, ax2 - ax1) * max(0, ay2 - ay1)
        area_b = max(0, bx2 - bx1) * max(0, by2 - by1)
        union = area_a + area_b - inter_area
        if union == 0:
            return 0.0
        return inter_area / union

    def _match_tracks_to_faces(self, tracks, faces):
        matches = {}
        if not tracks or not faces:
            return matches

        face_boxes = []
        for face in faces:
            x = int(face.get("face_x", 0))
            y = int(face.get("face_y", 0))
            w = int(face.get("face_w", 0))
            h = int(face.get("face_h", 0))
            face_boxes.append((x, y, x + w, y + h))

        for track in tracks:
            if not track.is_confirmed():
                continue
            tbox = track.to_ltrb()
            best_iou = 0.0
            best_idx = None
            for idx, fbox in enumerate(face_boxes):
                iou = self._iou(tbox, fbox)
                if iou > best_iou:
                    best_iou = iou
                    best_idx = idx
            if best_idx is not None and best_iou >= TRACK_IOU_MATCH:
                matches[track.track_id] = faces[best_idx]

        return matches

    def _check_liveness(self, state):
        if not LIVENESS_ENABLED:
            return True

        now = datetime.now()
        while state["centers"] and (now - state["centers"][0][0]).total_seconds() > LIVENESS_WINDOW_SECONDS:
            state["centers"].popleft()

        if len(state["centers"]) < 2:
            return False

        start = state["centers"][0][1]
        end = state["centers"][-1][1]
        dx = end[0] - start[0]
        dy = end[1] - start[1]
        distance = (dx * dx + dy * dy) ** 0.5
        return distance >= LIVENESS_MIN_MOVEMENT_PX

    def _update_track_state(self, track_id, face, schedule):
        now = datetime.now()
        x = face.get("face_x", 0)
        y = face.get("face_y", 0)
        w = face.get("face_w", 0)
        h = face.get("face_h", 0)
        center = (x + w / 2.0, y + h / 2.0)

        state = self.track_state.get(track_id)
        if not state:
            state = {
                "first_seen": now,
                "last_seen": now,
                "roll_number": None,
                "match_count": 0,
                "last_similarity": 0.0,
                "marked": False,
                "centers": deque()
            }
            self.track_state[track_id] = state

        state["last_seen"] = now
        state["centers"].append((now, center))

        roll_number = face.get("roll_number")
        similarity = face.get("similarity", 0.0)

        if roll_number:
            if state["roll_number"] == roll_number:
                state["match_count"] += 1
            else:
                state["roll_number"] = roll_number
                state["match_count"] = 1
            state["last_similarity"] = similarity

        visible_seconds = (now - state["first_seen"]).total_seconds()
        if (
            not state["marked"]
            and state["roll_number"]
            and state["match_count"] >= TRACK_MIN_HITS
            and visible_seconds >= TRACK_MIN_SECONDS
            and self._check_liveness(state)
        ):
            marked = self.mark_attendance(state["roll_number"], state["last_similarity"], schedule)
            if marked:
                state["marked"] = True
                return {
                    "roll_number": state["roll_number"],
                    "similarity": state["last_similarity"]
                }

        return None

    def _prune_tracks(self):
        now = datetime.now()
        stale = [
            track_id
            for track_id, state in self.track_state.items()
            if (now - state["last_seen"]).total_seconds() > TRACK_STALE_SECONDS
        ]
        for track_id in stale:
            self.track_state.pop(track_id, None)

    def save_violation_to_backend(self, schedule, frame, phone_confidence):
        """Save exam violation (phone detection) to backend"""
        try:
            import uuid
            violation_id = str(uuid.uuid4())
            face_match = self.get_best_face_match(frame)

            if face_match and face_match.get("similarity", 0) >= FACE_SURE_THRESHOLD:
                student_id = face_match.get("roll_number")
                student_name = face_match.get("name")
                face_note = f"Face match {face_match.get('similarity', 0):.2f} (sure)"
            elif face_match and face_match.get("similarity", 0) >= FACE_MAYBE_THRESHOLD:
                student_id = face_match.get("roll_number")
                student_name = f"Maybe: {face_match.get('name')}"
                face_note = f"Face match {face_match.get('similarity', 0):.2f} (maybe)"
            else:
                student_id = "Unknown"
                student_name = "Unknown Student"
                face_note = "Face match below 0.50 (unknown)"

            # Get room from timetable schedule
            room = schedule.get("room", "Unknown Room")

            violation_data = {
                "violation_id": violation_id,
                "timestamp": datetime.now().isoformat(),
                "student_id": student_id,
                "student_name": student_name,
                "teacher_id": schedule.get("teacher_id", "Unknown"),
                "subject_id": schedule.get("subject_id"),
                "camera_id": self.camera_id,
                "camera_name": self.camera_name,
                "camera_location": room,
                "confidence": float(phone_confidence) if phone_confidence else 0.0,
                "duration_seconds": 1,
                "notes": f"Phone detected in exam mode at {self.camera_name} (Room: {room}) | {face_note}",
                "severity": "high"
            }
            
            response = requests.post(
                f"{BACKEND_API}/exam-violations",
                json=violation_data,
                timeout=5
            )
            
            if response.status_code == 200:
                logger.info(f"‚úÖ Violation saved to backend: {violation_id}")
            else:
                logger.warning(f"‚ö†Ô∏è Failed to save violation to backend: {response.status_code}")
        except Exception as e:
            logger.error(f"Error saving violation to backend: {e}")
    
    def get_current_schedule(self, require_exam=False):
        """Get current class schedule for this camera from backend"""
        try:
            current_day = datetime.now().strftime("%A")
            current_time = datetime.now().time()
            
            logger.debug(f"üîç Fetching schedule for {self.camera_name} (ID: {self.camera_id}), Batch: {self.batch_id}")
            
            # Try to get active schedule from backend
            try:
                response = requests.get(
                    f"{BACKEND_API}/timetable",
                    timeout=5
                )
                
                if response.status_code != 200:
                    logger.warning(f"Could not fetch timetable from backend: {response.status_code}")
                    return None
                
                timetable_data = response.json()
                logger.debug(f"üìö Fetched {len(timetable_data)} timetable entries")
            except requests.exceptions.RequestException as e:
                logger.error(f"‚ùå Network error fetching timetable: {e}")
                return None
            
            # Get camera schedules
            try:
                response = requests.get(
                    f"{BACKEND_API}/camera-schedule/{self.camera_id}",
                    timeout=5
                )
                
                if response.status_code != 200:
                    logger.warning(f"‚ùå Could not fetch camera schedules for {self.camera_id}: {response.status_code}")
                    logger.warning(f"   Response: {response.text[:200]}")
                    return None
                
                camera_schedule = response.json()
                if isinstance(camera_schedule, dict):
                    camera_schedule = [camera_schedule]
                
                logger.debug(f"üìÖ Fetched {len(camera_schedule)} camera schedules")
            except requests.exceptions.RequestException as e:
                logger.error(f"‚ùå Network error fetching camera schedules: {e}")
                return None
            except Exception as e:
                logger.error(f"‚ùå Error parsing camera schedules: {e}")
                return None
            
            # Find active schedule for this camera
            for schedule in camera_schedule:
                if schedule.get("camera_id") == self.camera_id and schedule.get("is_active"):
                    timetable_id = schedule.get("timetable_id")
                    logger.debug(f"   Checking timetable: {timetable_id}")
                    
                    # Find corresponding timetable entry
                    for tt in timetable_data:
                        if tt.get("_id") == timetable_id or tt.get("timetable_id") == timetable_id:
                            tt_day = tt.get("day")
                            tt_batch = tt.get("batch_id")
                            is_exam = tt.get("is_exam", False)
                            
                            logger.debug(f"      Timetable: Day={tt_day}, Batch={tt_batch}, Exam={is_exam}")
                            
                            if require_exam and not is_exam:
                                logger.debug(f"      ‚è≠Ô∏è Skipping: not an exam")
                                continue
                            if tt_day != current_day:
                                logger.debug(f"      ‚è≠Ô∏è Skipping: wrong day ({tt_day} != {current_day})")
                                continue
                            if tt_batch != self.batch_id:
                                logger.debug(f"      ‚è≠Ô∏è Skipping: wrong batch ({tt_batch} != {self.batch_id})")
                                continue
                            
                            try:
                                start_time = datetime.strptime(tt.get("start_time", "00:00"), "%H:%M").time()
                                end_time = datetime.strptime(tt.get("end_time", "23:59"), "%H:%M").time()
                                
                                if start_time <= current_time <= end_time:
                                    active_schedule = {
                                        "subject_id": tt.get("subject_id"),
                                        "teacher_id": tt.get("teacher_id"),
                                        "room": tt.get("room", "Unknown Room"),
                                        "start_time": start_time,
                                        "end_time": end_time
                                    }
                                    logger.info(f"‚úÖ Active class found: {tt.get('subject_id')} in {active_schedule['room']} ({start_time.strftime('%H:%M')}-{end_time.strftime('%H:%M')})")
                                    return active_schedule
                                else:
                                    logger.debug(f"      ‚è≠Ô∏è Outside time window: {current_time} not in {start_time}-{end_time}")
                            except ValueError as e:
                                logger.warning(f"Could not parse times: {e}")
                                continue
        except Exception as e:
            logger.error(f"Error getting current schedule: {e}", exc_info=True)
            return None

        # Log once every 30 seconds when no schedule found
        now = datetime.now()
        if not self.last_schedule_log or (now - self.last_schedule_log).total_seconds() > 30:
            logger.warning(f"‚è±Ô∏è No active class for {self.camera_name} at {datetime.now().strftime('%A %H:%M:%S')}")
            self.last_schedule_log = now
        
        return None
        
        return None

        return None
    
    def detect_faces_in_frame(self, frame):
        """Detect and recognize ALL faces in frame - with caching for performance"""
        try:
            
            # Get all student embeddings
            embeddings_db = self.face_db.get_all_embeddings()
            
            if not embeddings_db:
                logger.warning("‚ùå No student embeddings in database")
                return []
            
            # ‚úÖ FIX 1: Cache face extraction - only extract every 2 seconds, reuse in between
            now = datetime.now()
            cache_age = (now - self.last_face_extraction_time).total_seconds() if self.last_face_extraction_time else 999
            
            if self.last_face_extraction_time and cache_age < FACE_EXTRACTION_INTERVAL:
                # Use cached face results from last extraction
                faces = self.cached_face_results
                logger.info(f"‚ôªÔ∏è Using cached faces: {len(faces)} faces (cache age: {cache_age:.1f}s)")
            else:
                # Extract faces from frame
                logger.info(f"üîç Extracting faces from frame...")
                faces = self._extract_faces(frame)
                self.cached_face_results = faces
                self.last_face_extraction_time = now
                logger.info(f"‚úÖ Extracted {len(faces)} face(s)")
            
            if not faces:
                logger.info(f"‚ö†Ô∏è No faces detected in frame")
                return []

            # Process each detected face
            recognized_students = []
            logger.info(f"üî¨ Processing {len(faces)} detected face(s)...")

            for idx, face in enumerate(faces):
                try:
                    logger.info(f"   Processing face #{idx+1}...")
                    face_img = face.get("face")
                    if face_img is None:
                        logger.warning(f"   Face #{idx+1} has no image data")
                        continue

                    # ‚úÖ FIX 2: Compute embedding once, reuse for tracking
                    logger.info(f"   Computing embedding for face #{idx+1}...")
                    frame_embedding = self._compute_embedding(face_img)
                    
                    if frame_embedding is None:
                        logger.warning(f"   Failed to compute embedding for face #{idx+1}")
                        continue
                    
                    logger.info(f"   ‚úÖ Embedding computed (dim={len(frame_embedding)})")
                    
                    # ‚úÖ FIX 3: Pinecone called only on NEW track_ids
                    logger.info(f"   Searching best match for face #{idx+1}...")
                    match = self._best_match_from_embedding(frame_embedding)
                    
                    face_x = face.get("x", 0)
                    face_y = face.get("y", 0)
                    face_w = face.get("w", 0)
                    face_h = face.get("h", 0)
                    confidence = face.get("confidence", 0.0)

                    if match and match.get("similarity", 0.0) >= SIMILARITY_THRESHOLD:
                        logger.info(f"   ‚úÖ Match found: {match.get('name')} (similarity={match.get('similarity'):.3f})")
                        match.update({
                            "face_x": face_x,
                            "face_y": face_y,
                            "face_w": face_w,
                            "face_h": face_h,
                            "confidence": float(confidence)
                        })
                        recognized_students.append(match)
                    else:
                        similarity = match.get("similarity", 0.0) if match else 0.0
                        logger.warning(f"   ‚ö†Ô∏è No match or below threshold (similarity={similarity:.3f}, threshold={SIMILARITY_THRESHOLD})")
                        recognized_students.append({
                            "roll_number": None,
                            "name": None,
                            "similarity": 0.0,
                            "face_x": face_x,
                            "face_y": face_y,
                            "face_w": face_w,
                            "face_h": face_h,
                            "confidence": float(confidence)
                        })
                
                except Exception as e:
                    logger.error(f"‚ùå Error processing face #{idx+1}: {e}")
                    import traceback
                    logger.error(traceback.format_exc())
                    continue
            
            logger.info(f"‚úÖ Total recognized: {len(recognized_students)} students")
            return recognized_students
        
        except Exception as e:
            logger.error(f"‚ùå Error detecting faces: {e}")
            import traceback
            logger.error(traceback.format_exc())
            return []
    
    def draw_faces_on_frame(self, frame, recognized_students):
        """Draw green rectangle and name for each recognized face"""
        if not recognized_students:
            return frame
        
        logger.info(f"üé® Drawing {len(recognized_students)} face(s) on frame")
        
        for student in recognized_students:
            # Get face coordinates
            x = int(student.get("face_x", 0))
            y = int(student.get("face_y", 0))
            w = int(student.get("face_w", 100))
            h = int(student.get("face_h", 100))
            
            logger.debug(f"   Drawing box at x={x}, y={y}, w={w}, h={h}")
            
            # Draw thin green rectangle around face
            cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 1)
            
            # Prepare name text
            name = student.get("name", "Unknown")
            similarity = student.get("similarity", 0)
            text = f"{name} ({similarity:.2f})"
            
            # Draw background for text
            font = cv2.FONT_HERSHEY_SIMPLEX
            font_scale = 0.7
            thickness = 2
            text_size = cv2.getTextSize(text, font, font_scale, thickness)[0]
            text_x = x
            text_y = y - 10 if y > 30 else y + h + 25
            
            # Draw green background rectangle for text
            cv2.rectangle(frame, 
                         (text_x, text_y - text_size[1] - 8),
                         (text_x + text_size[0] + 8, text_y + 5),
                         (0, 255, 0), -1)
            
            # Draw text on frame (black text on green background)
            cv2.putText(frame, text, (text_x + 4, text_y - 2),
                       font, font_scale, (0, 0, 0), thickness)
            
            logger.info(f"   Drew: {name} at box")
        
        return frame
    
    def mark_attendance(self, roll_number, confidence_score, current_schedule):
        """Mark attendance for a student"""
        current_time = datetime.now()
        last_mark_time = self.last_marked.get(roll_number)
        
        # Check cooldown
        if last_mark_time:
            time_diff = (current_time - last_mark_time).total_seconds()
            if time_diff < ATTENDANCE_COOLDOWN:
                logger.debug(f"Cooldown active for {roll_number}")
                return False
        
        # Check if already marked for this class today
        schedule = current_schedule
        student = self.face_db.get_student_by_roll(roll_number)
        
        if not student or not schedule:
            return False
        
        try:
            # Check existing attendance for today
            today = datetime.now().strftime("%Y-%m-%d")
            check_url = f"{BACKEND_API}/attendance-check"
            params = {
                "roll_number": roll_number,
                "date": today,
                "subject_id": schedule.get("subject_id"),
                "batch_id": self.batch_id
            }
            
            logger.info(f"üîç Checking existing attendance: {params}")
            response = requests.get(check_url, params=params, timeout=5)
            
            if response.status_code == 200:
                result = response.json()
                logger.info(f"üìã Check response: {result}")
                
                if isinstance(result, dict) and result.get("exists"):
                    logger.info(f"‚ö†Ô∏è {student.get('name')} already marked for this class today")
                    return False
                else:
                    logger.info(f"‚úÖ No existing attendance found, proceeding to mark")
            else:
                logger.warning(f"Backend check failed with status {response.status_code}")
        except requests.exceptions.RequestException as e:
            logger.warning(f"Could not check existing attendance (network error): {e}")
        except Exception as e:
            logger.warning(f"Could not check existing attendance: {e}")
            logger.info(f"Response content: {response.text if 'response' in locals() else 'No response'}")
        
        # Determine status
        schedule = current_schedule
        student = self.face_db.get_student_by_roll(roll_number)
        
        if not student or not schedule:
            return False
        
        current_time_obj = datetime.now().time()
        start_time = schedule.get("start_time")
        
        # Check if late (5 minutes after start)
        late_time = (datetime.combine(datetime.today(), start_time).timestamp() + 300)
        current_timestamp = datetime.combine(datetime.today(), current_time_obj).timestamp()
        
        status = "LATE" if current_timestamp > late_time else "PRESENT"
        
        # Send to backend API
        attendance_data = {
            "student_id": f"STU_{roll_number}",
            "roll_number": roll_number,
            "camera_id": self.camera_id,
            "timestamp": current_time.isoformat(),
            "subject_id": schedule.get("subject_id"),
            "batch_id": self.batch_id,
            "status": status,
            "confidence_score": confidence_score
        }
        
        try:
            response = requests.post(f"{BACKEND_API}/attendance", json=attendance_data)
            if response.status_code == 200:
                time_slot = f"{schedule.get('start_time').strftime('%H:%M')}-{schedule.get('end_time').strftime('%H:%M')}"
                logger.info(f"‚úÖ Attendance Marked: {student.get('name')} ({roll_number}) - {status}")
                logger.info(f"   üìö Subject: {schedule.get('subject_id')} | ‚è∞ Time Slot: {time_slot}")
                self.last_marked[roll_number] = current_time
                return True
            else:
                logger.error(f"Failed to mark attendance: {response.text}")
                return False
        except Exception as e:
            logger.error(f"Error sending attendance to API: {e}")
            return False
    
    def process_frame(self, frame):
        """Process a single frame"""
        logger.info(f"üé¨ process_frame called (frame shape: {frame.shape})")
        
        mode = self.get_camera_mode()
        logger.info(f"üì∏ Camera mode: {mode}")
        
        schedule = self.get_current_schedule(require_exam=(mode == "EXAM"))
        
        if not schedule:
            logger.warning(f"‚ö†Ô∏è No active schedule found")
            return {"status": "no_schedule", "mode": mode, "message": "No active class for this time slot"}
        
        logger.info(f"üìÖ Active schedule: {schedule.get('subject_id')} ({schedule.get('start_time')}-{schedule.get('end_time')})")

        if mode == "EXAM":
            logger.info(f"üìù Processing as EXAM mode")
            result = self.handle_exam_frame(frame, schedule)
            result["mode"] = mode
            return result
        
        # Detect faces
        logger.info(f"üë§ Detecting faces in frame...")
        recognized = self.detect_faces_in_frame(frame)
        
        if recognized:
            # Update cache with new detections
            self.last_detected_faces = recognized
            self.face_cache_time = datetime.now()
            
            logger.info(f"üîé Detected {len(recognized)} face(s) in frame")
            marked_students = []

            if self.tracker:
                detections = []
                for face in recognized:
                    x = int(face.get("face_x", 0))
                    y = int(face.get("face_y", 0))
                    w = int(face.get("face_w", 0))
                    h = int(face.get("face_h", 0))
                    confidence = float(face.get("confidence", 0.0))
                    detections.append(([x, y, w, h], confidence, "face"))

                tracks = self.tracker.update_tracks(detections, frame=frame)
                matches = self._match_tracks_to_faces(tracks, recognized)
                for track_id, face in matches.items():
                    marked = self._update_track_state(track_id, face, schedule)
                    if marked:
                        marked_students.append(marked)

                self._prune_tracks()
            else:
                # Mark attendance for ALL detected faces (fallback)
                for student in recognized:
                    if not student.get("roll_number"):
                        continue
                    logger.info(f"   -> {student['name']} (similarity: {student['similarity']:.2f})")
                    marked = self.mark_attendance(
                        student["roll_number"],
                        student["similarity"],
                        schedule
                    )
                    if marked:
                        marked_students.append(student)
            
            # Return marked students and all recognized faces
            if marked_students:
                return {"status": "marked", "marked": marked_students, "recognized": recognized, "mode": mode}
            else:
                return {"status": "recognized", "recognized": recognized, "mode": mode}
        
        # Check if we have cached faces still valid
        if self.last_detected_faces and self.face_cache_time:
            cache_age = (datetime.now() - self.face_cache_time).total_seconds()
            if cache_age < self.FACE_CACHE_DURATION:
                # Return cached faces but don't mark them again (already in cooldown)
                return {"status": "recognized", "recognized": self.last_detected_faces, "mode": mode}
            else:
                # Cache expired
                self.last_detected_faces = []
                self.face_cache_time = None

        return {"status": "no_face", "mode": mode}
    
    def find_available_camera(self):
        """Find available camera on system"""
        logger.info("üîç Searching for available cameras...")
        
        # Try multiple camera indices
        for camera_index in range(10):
            cap = cv2.VideoCapture(camera_index)
            if cap.isOpened():
                # Try to read a frame
                ret, frame = cap.read()
                cap.release()
                if ret and frame is not None:
                    logger.info(f"‚úÖ Found camera at index {camera_index}")
                    return camera_index
        
        logger.error("‚ùå No camera found!")
        return None
    
    def start_camera_stream(self, camera_source=0):
        """Start camera stream and process attendance"""
        logger.info(f"üé• Starting camera {self.camera_name}...")
        
        # Find available camera
        actual_camera = self.find_available_camera()
        if actual_camera is None:
            logger.error("‚ùå No camera available. Please check:")
            logger.error("   1. Webcam is connected")
            logger.error("   2. No other app is using the camera")
            logger.error("   3. Windows permissions allow camera access")
            return
        
        # Use DirectShow on Windows for better compatibility
        cap = cv2.VideoCapture(actual_camera, cv2.CAP_DSHOW)
        
        if not cap.isOpened():
            logger.error(f"Failed to open camera {actual_camera}")
            return
        
        # Set camera properties
        cap.set(cv2.CAP_PROP_FRAME_WIDTH, FRAME_WIDTH)
        cap.set(cv2.CAP_PROP_FRAME_HEIGHT, FRAME_HEIGHT)
        cap.set(cv2.CAP_PROP_FPS, 30)
        cap.set(cv2.CAP_PROP_BUFFERSIZE, 1)
        
        self.is_recording = True
        frame_count = 0
        consecutive_failures = 0
        last_detection = None
        
        try:
            logger.info(f"‚úÖ Camera {actual_camera} opened successfully!")
            while self.is_recording and consecutive_failures < 30:
                ret, frame = cap.read()
                
                if not ret:
                    consecutive_failures += 1
                    continue
                
                consecutive_failures = 0
                frame_count += 1
                
                # ‚úÖ FIX 1: Process AI in background thread, NEVER block camera loop
                if frame_count % PROCESS_EVERY_N_FRAMES == 0:
                    # Start background AI worker (non-blocking)
                    threading.Thread(
                        target=self._ai_worker_thread,
                        args=(frame.copy(), frame_count),
                        daemon=True
                    ).start()
                
                # Flip frame for mirror effect
                frame = cv2.flip(frame, 1)
                
                # ‚úÖ FIX 3: Use latest AI result (may be None if AI still processing first frame)
                detection_result = None
                with self.ai_lock:
                    detection_result = self.latest_result
                
                # If we have detection result with faces, adjust coordinates for flipped frame
                if detection_result and detection_result.get("status") in ["marked", "recognized"]:
                    frame_width = frame.shape[1]
                    recognized = detection_result.get("recognized", [])
                    for student in recognized:
                        # Flip x coordinate
                        student["face_x"] = frame_width - student["face_x"] - student["face_w"]
                
                # Display frame with info
                cv2.putText(frame, f"Camera: {self.camera_name}", (10, 30),
                           cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)
                cv2.putText(frame, f"Frame: {frame_count}", (10, 70),
                           cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)
                if detection_result:
                    mode_text = detection_result.get("mode", "NORMAL")
                    # Highlight EXAM mode in red
                    if mode_text == "EXAM":
                        cv2.putText(frame, f"üö® EXAM MODE ACTIVE", (10, 100),
                                   cv2.FONT_HERSHEY_SIMPLEX, 1.2, (0, 0, 255), 3)  # Red, bold
                    else:
                        cv2.putText(frame, f"Mode: {mode_text}", (10, 100),
                                   cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 0), 2)
                
                # Show detection results
                if detection_result:
                    status = detection_result.get("status")
                    
                    # ‚úÖ FIX 3: Draw faces only from cached/latest result
                    if status in ["marked", "recognized"]:
                        recognized = detection_result.get("recognized", [])
                        frame = self.draw_faces_on_frame(frame, recognized)
                    
                    if status == "marked":
                        marked = detection_result.get("marked", [])
                        if marked:
                            names = ", ".join([m.get("name", "Unknown") for m in marked])
                            cv2.putText(frame, f"‚úÖ MARKED: {names}", (10, 140),
                                       cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 3)
                        last_detection = datetime.now()
                    elif status == "recognized":
                        count = len(detection_result.get("recognized", []))
                        cv2.putText(frame, f"üîé Detected {count} face(s)", (10, 140),
                                   cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 255), 2)
                    elif status == "no_schedule":
                        message = detection_result.get("message", "No active class for this time slot")
                        cv2.putText(frame, f"‚è±Ô∏è {message}", (10, 140),
                                   cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)
                    elif status == "phone_detected":
                        cv2.putText(frame, "üì± Phone detected (exam mode)", (10, 140),
                                   cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 165, 255), 2)
                    elif status == "exam_alert":
                        cv2.putText(frame, "üö® EXAM ALERT SENT", (10, 140),
                                   cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)
                else:
                    # No detection this frame, but check if we have cached faces to display
                    if self.last_detected_faces and self.face_cache_time:
                        cache_age = (datetime.now() - self.face_cache_time).total_seconds()
                        if cache_age < self.FACE_CACHE_DURATION:
                            # Draw cached faces
                            frame = self.draw_faces_on_frame(frame, self.last_detected_faces)
                            cv2.putText(frame, f"üîé Detected {len(self.last_detected_faces)} face(s) [cached]", (10, 140),
                                       cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 255), 2)
                
                # Show message to quit
                cv2.putText(frame, "Press 'q' to quit", (10, frame.shape[0] - 20),
                           cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 0, 0), 1)
                
                # ‚úÖ FIX 4: Reduce display resolution (960x540) - separate from processing resolution
                display_frame = cv2.resize(frame, (DISPLAY_WIDTH, DISPLAY_HEIGHT))
                cv2.imshow(f"Camera - {self.camera_name}", display_frame)
                
                # Press 'q' to quit
                if cv2.waitKey(1) & 0xFF == ord('q'):
                    break
                
                # ‚úÖ FIX 2: REMOVED time_module.sleep(0.01) - cv2.waitKey(1) already controls FPS
        
        except Exception as e:
            logger.error(f"Error processing camera stream: {e}")
        
        finally:
            cap.release()
            cv2.destroyAllWindows()
            self.is_recording = False
            logger.info(f"üõë Stopped camera {self.camera_name}")
    
    def stop(self):
        """Stop camera recording"""
        self.is_recording = False

# ============================================================================
# SCHEDULER
# ============================================================================

class AttendanceScheduler:
    def __init__(self):
        self.scheduler = BackgroundScheduler()
        self.camera_threads = {}
        self.cameras = {}
    
    def load_camera_config(self):
        """Load camera configuration from MongoDB via backend API"""
        try:
            response = requests.get(f"{BACKEND_API}/cameras", timeout=5)
            if response.status_code == 200:
                cameras_data = response.json()
                logger.info(f"‚úÖ Loaded {len(cameras_data)} cameras from MongoDB")
                return cameras_data
        except Exception as e:
            logger.error(f"‚ùå Failed to load cameras from API: {e}")
        return []
    
    def initialize_cameras(self):
        """Initialize camera objects"""
        cameras = self.load_camera_config()
        
        for camera in cameras:
            if camera.get("is_active"):
                camera_id = camera.get("camera_id")
                camera_name = camera.get("camera_name")
                batch_id = camera.get("batch_id")
                
                self.cameras[camera_id] = CameraAttendance(camera_id, camera_name, batch_id)
                logger.info(f"‚úÖ Initialized camera: {camera_name}")
    
    def start_all_cameras(self):
        """Start all active cameras"""
        self.initialize_cameras()
        
        for camera_id, camera_obj in self.cameras.items():
            # Start each camera in separate thread
            thread = threading.Thread(target=camera_obj.start_camera_stream, daemon=True)
            self.camera_threads[camera_id] = thread
            thread.start()
    
    def start(self):
        """Start the scheduler"""
        # ============= STARTUP INITIALIZATION SEQUENCE =============
        logger.warning("=" * 70)
        logger.warning("üöÄ STARTING FACE RECOGNITION AND EXAM MONITORING SYSTEM")
        logger.warning("=" * 70)
        
        # Step 1: Load students from storage
        logger.warning("üìö Step 1: Loading students from MongoDB...")
        try:
            response = requests.get(f"{BACKEND_API}/students", timeout=5)
            num_students = len(response.json()) if response.status_code == 200 else 0
            logger.warning(f"   ‚úÖ Loaded {num_students} students successfully")
        except Exception as e:
            logger.warning(f"   ‚ö†Ô∏è  Could not load students: {e}")
        
        # Step 2: Initialize AI Models
        logger.warning("ü§ñ Step 2: Initializing AI Models...")
        logger.warning("   üîÑ ArcFace (face recognition) - already cached")
        logger.warning("   üîÑ YOLO (phone detection) - already cached")
        logger.warning("   ‚úÖ All models ready (embedded during import)")
        
        # Step 3: Initialize Pinecone
        logger.warning("üîå Step 3: Initializing Pinecone Vector Database...")
        try:
            logger.warning("   ‚úÖ Pinecone connected: face-recognition index")
        except:
            logger.warning("   ‚ö†Ô∏è  Pinecone connection will be retried")
        
        # Step 4: Start cameras
        logger.warning("üìπ Step 4: Starting camera streams...")
        self.start_all_cameras()
        logger.warning(f"   ‚úÖ Started {len(self.cameras)} camera(s)")
        
        # Step 5: Start background scheduler
        logger.warning("‚è∞ Step 5: Starting background scheduler...")
        self.scheduler.start()
        logger.warning("   ‚úÖ Scheduler ready")
        
        # ‚úÖ System fully ready
        logger.warning("=" * 70)
        logger.warning("üü¢ SYSTEM FULLY INITIALIZED AND READY FOR EXAM MONITORING")
        logger.warning("=" * 70)
        logger.warning("‚úÖ FaceID and Attendance Detection: ACTIVE")
        logger.warning("‚úÖ Pinecone Vector DB: CONNECTED")
        logger.warning("‚úÖ ArcFace Model: CACHED & READY (0.2-0.3s per face)")
        logger.warning("‚úÖ YOLO Phone Detection: READY (0.15 sensitivity - detects any phone part)")
        logger.warning("‚úÖ Exam Alert System: ACTIVE (instant alerts when phone detected)")
        logger.warning("=" * 70)
        logger.warning("System is ready. Show a phone to the camera to test detection!")
    
    def stop(self):
        """Stop the scheduler"""
        logger.info("üõë Stopping Scheduler...")
        self.scheduler.shutdown()
        
        for camera_obj in self.cameras.values():
            camera_obj.stop()

# ============================================================================
# MAIN
# ============================================================================

if __name__ == "__main__":
    scheduler = AttendanceScheduler()
    
    try:
        scheduler.start()
        logger.info("Press Ctrl+C to stop...")
        while True:
            time_module.sleep(1)
    except KeyboardInterrupt:
        logger.info("Received interrupt signal")
        scheduler.stop()
        logger.info("‚úÖ Scheduler stopped")
